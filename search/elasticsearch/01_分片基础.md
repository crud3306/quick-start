

1 Es对复杂分布式机制的透明隐藏特性
=============
分片机制

集群发现机制 (cluster discovery)

shard负载均衡
假设有3个节点，总共有25个shard要分配到3个节点上去，es会自动进行均匀分配，以保持每个节点的均衡的读写负载请求

shard副本、请求路由、集群扩容、shard重新分配


2 Es的垂直扩容与水平扩容
=============
垂直扩容

水平扩容



3 增加或减少节点时数据rebalance
=============


4 master节点
=============
创建或删除索引
增加或删除节点


管理es集群的元数据：
	索引创建和删除，维护索引元数据；
	节点的增加和移阶乘，维护集群元数据
默认情况下，会自动选择出一台节点作为master节点

master节点不承载所有的请求，所以不会是一个单点瓶颈


5 节点对等的分布式架构
=============
1）节点对等，每个节点都能接收所有的请求

2）自动请求路由

3）响应收集

通俗的讲：可以通过任意节点发送请求，如果查询数据在该节点则直接返回；如果不在该节点上时，则把请求路由到真实的存在数据的节点上，收到真实节点的响应数据后，然后再返回。





shard & replica机制梳理
=============
1) index包含多个shard

2) 每个shard都是一个最小工作单元，承载部分数据，每个shard都是一个lucene实例，有完整的建立索引和处理请求的能力

3) 增减节点时，shard会自动在nodes中负载均衡

4) primary shard和replica shard，每个document肯定只存在于某一个primary shard以及基对应的replica shard中，不可能存在于多个primary shard中

5) replica shard是primary shard的副本，负责容错，以及承担读请求负载

6) primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改

7) primary shard的默认数量是5，每个primary shard对应的replica默认是1。所以总共默认有10个shard，5个primary shard，5个replica shard

8) primary shard不能和自已的replica shard放在同一个节点上(否则节点宕机，primary shard和副本都丢失，起不到容错的作用)，但是可以和其他primary shard的replica shard放在同一个节点上



单node环境下创建index是什么样子的
================
1) 假设单node环境下，我们创建一个index，指定有3个primary shard，3个replica shard

2）集群的status是yellow

3）这个时候，只会将3个primary shard分配到仅有的一个node上去，另外3个replica shard是无法分配的

4）这种单node集群可以正常工作，但是一量出现节点宕机，数据全部丢失，而且集群不可用，无法承接任何请求

```
PUT /test_index
{
	"settings":{
		"number_of_shards":3,
		"number_of_replicas":1
	}
}
```


2个node的环境下，replica shard是如何分配的
===============
假设单node环境下，我们创建一个index，指定有3个primary shard，3个replica shard

一个节点放3个primary shard，另一个节点放3个replica shard



横向扩容，如何超出极限，以及如何提升容错性
===============
假设有6个shard，3个primary，3个replica

1) primary & replica 自动负载均衡，6个shard，3个primary，3个replica

2) 每个node有更少的shard，IO/CPU/Memory资源给每个shard分配更多，每个shard性能更好

3) 扩容的极限，6个shard(3个primary，3个replica)，最多扩容到6台机器，每个shard可以占用单台服务器的所有资源，性能最好

4) 超出扩容极限，动态修改replica数量，9个shard(3primary, 6replica)，扩容到9台机器，相比3台机器时，拥有3倍的吞吐量

5）3台机器下，9个shard(3 primary, 6 replica)，资源更少，但容错性更好，最多容纳2台机器宕机，6个shard只能容纳1台机器宕机


ES容错机制：master选举，replica容错，数据恢复
================
假设9个shard，3个primary, 6个replica

1) master node宕机，自动master选择，状态red
2) replica容错：新master将replica提升为primary shard，状态为yellow
3) 重启宕机的node，新master copy replica到该node，使用原有的shard并同步宕机后的修改，状态为green

具体场景如下：
```
node1: p0 r1 r2  
node2: p1 r0 r2  
node3: p2 r0 r1  

假设node1宕机，则p0 r1 r2三个shard丢失
cluster status = red
原因：master宕机的一瞬间，p0这个primary shard就没了，此时就不是active status，就不是所有的primary shard都是active了

容错第一步：
master选举 
自动选举另外一个node成为新的master，承担起master的责任来

容错第二步：
新master将丢失掉的primary shard的某个replica shard提升为primary shard。此时cluster status会变为yellow，因为primary shard全都变成active了。但量少了一个replica shard，所以不是所有的replica shard都是active了。


容错第三步：
重启故障的node，new master会将缺失的副本都copy一份到该node上去，而且该node会使用之前已有的shard数据，只是同步一下宕机之后发生过的修改。
```


 元数据 _index元数据、_type元数据、_id元数据
===========
```
_index元数据
1）代表一个document存放在哪个index中
2）类似的数据放在一个索引，非类似的数据放在不同索引
3）index中包含了很多类似的document
4）索引名称必须是小写的，不能用下划线开头，不能包含逗号
```

```
_type元数据
1）代表document属于index中的哪个类别(type)
2）一个索引通常会划分为多个type，逻辑上对index中有些许不同的几类数据进行分类
3）type名称可以是大写或者小写，但是同时不能用下划线开并头，不能包含逗号
```

```
_id元数据
1) 代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document
2) 我们可以手动指定document的id，也可以不指定，由es自动为我们创建一个id
```



